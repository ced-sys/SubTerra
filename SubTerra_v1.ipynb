{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+dFcjcAFHQTtZTZCDa8jb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ced-sys/SubTerra/blob/main/SubTerra_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiXFhIX01D9h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from xgboost import XGBClassifier\n",
        "  XGBOOST_AVAILABLE=True\n",
        "except ImportError:\n",
        "  XGBOOST_AVAILABLE=False\n",
        "  print(\"Warning: XGBoost not available. Install with: pip install xgboost\")"
      ],
      "metadata": {
        "id": "sDA2s8Dn2Gp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model={}\n",
        "X_train=None\n",
        "X_test=None\n",
        "y_train=None\n",
        "y_test=None\n",
        "feature_names=None\n",
        "scaler=StandardScaler()"
      ],
      "metadata": {
        "id": "lWEo9fi05cla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(filepath='/content/training_dataset.csv'):\n",
        "  global feature_names\n",
        "\n",
        "  try:\n",
        "    print(f\"Loading dataset from: {filepath}\")\n",
        "    df=pd.read_csv(filepath)\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "    #Remove non-feature columns\n",
        "    drop_cols=[col for col in df.columns\n",
        "               if any (keyword in col.lower()\n",
        "               for keyword in ['fid', 'path', 'layer', 'id'])]\n",
        "\n",
        "    if drop_cols:\n",
        "      print(f\"Dropping columns: {drop_cols}\")\n",
        "      df_cleaned=df.drop(columns=drop_cols)\n",
        "    else:\n",
        "      df_cleaned=df.copy()\n",
        "\n",
        "    #Separate features and labels\n",
        "    if 'label' not in df_cleaned.columns:\n",
        "      raise ValueError(\"No 'label' column found in dataset\")\n",
        "\n",
        "    X=df_cleaned.drop(columns=['label'])\n",
        "    y=df_cleaned['label']\n",
        "\n",
        "    #convert to numeric and handle missing values\n",
        "    X=X.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    #Handle missing values\n",
        "    missing_counts=X.isnull().sum()\n",
        "    if missing_counts.any():\n",
        "      print(f\"Missing values found: {missing_counts[missing_counts>0]}\")\n",
        "      X=X.fillna(X.median())\n",
        "\n",
        "    feature_names=list(X.columns)\n",
        "    print(f\"Features: {len(feature_names)}\")\n",
        "    print(f\"Label distribution: {y.value_counts().to_dict()}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    raise FileNotFoundError(f\"Dataset file not found: {filepath}\")\n",
        "  except Exception as e:\n",
        "    raise Exception(f\"Error loading dataset: {str(e)}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HdQVhIuP5out"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(X, y, test_size=0.25, random_state=42):\n",
        "  global X_train, X_test, y_train, y_test, scaler\n",
        "\n",
        "  X_train, X_test, y_train, y_test=train_test_split(\n",
        "      X, y, test_size=test_size, random_state=random_state,\n",
        "      stratify=y, shuffle=True\n",
        "  )\n",
        "\n",
        "  #Scale features for better perfoemance\n",
        "  X_train_scaled=scaler.fit_transform(X_train)\n",
        "  X_test_scaled=scaler.transform(X_test)\n",
        "\n",
        "  print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "  print(f\"Testing set: {X_test.shape[0]} samples\")\n",
        "\n",
        "  return X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled"
      ],
      "metadata": {
        "id": "Ofrt9WQl9tcr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_models(X_train, y_train, X_train_scaled, random_state=42):\n",
        "  global models\n",
        "\n",
        "  print(\"\\nTraining models...\")\n",
        "\n",
        "  #Decision tree\n",
        "  models['decision_tree']=DecisionTreeClassfier(\n",
        "      max_depth=8,\n",
        "      min_samples_split=10,\n",
        "      min_samples_leaf=5,\n",
        "      random_state=random_state\n",
        "  )\n",
        "\n",
        "  #Random forest\n",
        "  models['random_forest']=RandomForestClassifier(\n",
        "      n_estimators=100,\n",
        "      max_depth=10,\n",
        "      min_samples_split=10,\n",
        "      random_state=random_state,\n",
        "      n_jobs=-1\n",
        "  )\n",
        "\n",
        "  #XGBoost\n",
        "  if XGBOOST_AVAILABLE:\n",
        "    models['wgboost']=XGBClassifer(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        random_state=random_state,\n",
        "        eval_metric='logloss',\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "\n",
        "  #Train all models\n",
        "  for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    if name == 'xgbost':\n",
        "      model.fit(X_train_scaled, y_train)\n",
        "    else:\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "  print(\"All models trained successfully\")\n",
        "  return models"
      ],
      "metadata": {
        "id": "dMT_-78kiuco"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oqK2UetIlgQt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}