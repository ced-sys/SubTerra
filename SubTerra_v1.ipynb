{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNL7kjCz5IQUBDCcaNFC/7I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ced-sys/SubTerra/blob/main/SubTerra_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tiXFhIX01D9h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from xgboost import XGBClassifier\n",
        "  XGBOOST_AVAILABLE=True\n",
        "except ImportError:\n",
        "  XGBOOST_AVAILABLE=False\n",
        "  print(\"Warning: XGBoost not available. Install with: pip install xgboost\")"
      ],
      "metadata": {
        "id": "sDA2s8Dn2Gp8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model={}\n",
        "X_train=None\n",
        "X_test=None\n",
        "y_train=None\n",
        "y_test=None\n",
        "feature_names=None\n",
        "scaler=StandardScaler()"
      ],
      "metadata": {
        "id": "lWEo9fi05cla"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(filepath='/content/training_dataset.csv'):\n",
        "  global feature_names\n",
        "\n",
        "  try:\n",
        "    print(f\"Loading dataset from: {filepath}\")\n",
        "    df=pd.read_csv(filepath)\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "    #Remove non-feature columns\n",
        "    drop_cols=[col for col in df.columns\n",
        "               if any (keyword in col.lower()\n",
        "               for keyword in ['fid', 'path', 'layer', 'id'])]\n",
        "\n",
        "    if drop_cols:\n",
        "      print(f\"Dropping columns: {drop_cols}\")\n",
        "      df_cleaned=df.drop(columns=drop_cols)\n",
        "    else:\n",
        "      df_cleaned=df.copy()\n",
        "\n",
        "    #Separate features and labels\n",
        "    if 'label' not in df_cleaned.columns:\n",
        "      raise ValueError(\"No 'label' column found in dataset\")\n",
        "\n",
        "    X=df_cleaned.drop(columns=['label'])\n",
        "    y=df_cleaned['label']\n",
        "\n",
        "    #convert to numeric and handle missing values\n",
        "    X=X.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    #Handle missing values\n",
        "    missing_counts=X.isnull().sum()\n",
        "    if missing_counts.any():\n",
        "      print(f\"Missing values found: {missing_counts[missing_counts>0]}\")\n",
        "      X=X.fillna(X.median())\n",
        "\n",
        "    feature_names=list(X.columns)\n",
        "    print(f\"Features: {len(feature_names)}\")\n",
        "    print(f\"Label distribution: {y.value_counts().to_dict()}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    raise FileNotFoundError(f\"Dataset file not found: {filepath}\")\n",
        "  except Exception as e:\n",
        "    raise Exception(f\"Error loading dataset: {str(e)}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HdQVhIuP5out"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(X, y, test_size=0.25, random_state=42):\n",
        "  global X_train, X_test, y_train, y_test, scaler\n",
        "\n",
        "  X_train, X_test, y_train, y_test=train_test_split(\n",
        "      X, y, test_size=test_size, random_state=random_state,\n",
        "      stratify=y, shuffle=True\n",
        "  )\n",
        "\n",
        "  #Scale features for better perfoemance\n",
        "  X_train_scaled=scaler.fit_transform(X_train)\n",
        "  X_test_scaled=scaler.transform(X_test)\n",
        "\n",
        "  print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "  print(f\"Testing set: {X_test.shape[0]} samples\")\n",
        "\n",
        "  return X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled"
      ],
      "metadata": {
        "id": "Ofrt9WQl9tcr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_models(X_train, y_train, X_train_scaled, random_state=42):\n",
        "  global models\n",
        "\n",
        "  print(\"\\nTraining models...\")\n",
        "\n",
        "  #Decision tree\n",
        "  models['decision_tree']=DecisionTreeClassfier(\n",
        "      max_depth=8,\n",
        "      min_samples_split=10,\n",
        "      min_samples_leaf=5,\n",
        "      random_state=random_state\n",
        "  )\n",
        "\n",
        "  #Random forest\n",
        "  models['random_forest']=RandomForestClassifier(\n",
        "      n_estimators=100,\n",
        "      max_depth=10,\n",
        "      min_samples_split=10,\n",
        "      random_state=random_state,\n",
        "      n_jobs=-1\n",
        "  )\n",
        "\n",
        "  #XGBoost\n",
        "  if XGBOOST_AVAILABLE:\n",
        "    models['wgboost']=XGBClassifer(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        random_state=random_state,\n",
        "        eval_metric='logloss',\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "\n",
        "  #Train all models\n",
        "  for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    if name == 'xgbost':\n",
        "      model.fit(X_train_scaled, y_train)\n",
        "    else:\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "  print(\"All models trained successfully\")\n",
        "  return models"
      ],
      "metadata": {
        "id": "dMT_-78kiuco"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_models(X_test, y_test, X_test_scaled):\n",
        "  print(\"\\n\"+\"=\"*60)\n",
        "  print(\"MODEL EVALUATION RESULTS\")\n",
        "  print(\"=\"*60)\n",
        "\n",
        "  results={}\n",
        "\n",
        "  for name, model in models.items():\n",
        "    print(f\"\\n--- {name.upper().replace('_', ' ')} ---\")\n",
        "\n",
        "    #Make predictions\n",
        "    if name == 'xgboost':\n",
        "      y_pred=model.predict(X_test_scaled)\n",
        "    else:\n",
        "      y_pred=model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy=accuracy_score(y_test, y_pred)\n",
        "    results[name]=accuracy\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # COnfusion matrix\n",
        "    cm=confusion_matrix(y_test, y_pred)\n",
        "    print(f\"Confusion Matrix: \\n{cm}\")\n",
        "\n",
        "  #Best model\n",
        "  best_model=max(results, key=results.get)\n",
        "  print(f\"\\nBest performing model: {best_model} (Accuracy: {results[best_model]:.4f})\")\n",
        "  return results"
      ],
      "metadata": {
        "id": "oqK2UetIlgQt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature_importance(top_n=15):\n",
        "  n_models=len(models)\n",
        "  fig, axes=plt.subplots(1, n_models, figsize=(6*n_models, 8))\n",
        "\n",
        "  if n_models==1:\n",
        "    axes=[axes]\n",
        "\n",
        "  for idx, (name, model) in enumerate(models.items()):\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "      importances=model.feature_importances_\n",
        "      indices=np.argsort(importances)[::-1][:top_n]\n",
        "\n",
        "      ax=axes[idx]\n",
        "      ax.barh(range(len(indices)), importances[indices])\n",
        "      ax.set_yticks(range(len(indices)))\n",
        "      ax.set_yticklabels([feature_names[i] for i in indices])\n",
        "      ax.set_xlabel('Feature Importance')\n",
        "      ax.set_title(f'{name.replace(\"_\", \" \").title()}\\nTop {top_n} Features')\n",
        "      ax.invert_yaxis()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "i5BA7dnM6a2e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_decision_tree():\n",
        "  if 'decision_tree' in models:\n",
        "    print(\"\\n\"+\"=\"*60)\n",
        "    print(\"DECISION TREE RULES\")\n",
        "    print(\"=\"*60)\n",
        "    tree_rules=export_text(\n",
        "        models['decision_tree'],\n",
        "        feature_names=feature_names,\n",
        "        max_depth=5\n",
        "    )\n",
        "    print(tree_rules)\n",
        "  else:\n",
        "    print(\"Decision tree not available for explanation\")"
      ],
      "metadata": {
        "id": "z2qt4hjO63gy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_sample(features, model_name='decision_tree'):\n",
        "  if model_name not in models:\n",
        "    raise ValueError(f\"Model {model_name} not availbale\")\n",
        "\n",
        "  model=models[model_name]\n",
        "\n",
        "  #Convert features to array\n",
        "  if isinstance(features, dict):\n",
        "    feature_array=np.array([features.get(name, 0) for name in feature_names])\n",
        "  else:\n",
        "    feature_array=np.array(features)\n",
        "\n",
        "  feature_array=feature_array.reshape(1, -1)\n",
        "\n",
        "  #Scale if needed\n",
        "  if model_name=='xgboost':\n",
        "    feature_array=scaler.transform(feature_array)\n",
        "\n",
        "  #Make prediction\n",
        "  prediction=model.predict(feature_array)[0]\n",
        "\n",
        "  #Get probability if available\n",
        "  if hasattr(model, 'predict_proba'):\n",
        "    if model_name=='xgboost':\n",
        "      probabilities=model.predict_proba(feature_array.reshape(1, -1))[0]\n",
        "    else:\n",
        "      probabilities=None\n",
        "\n",
        "    return {\n",
        "        'prediction': int(prediction),\n",
        "        'prediction_label': 'Positive' if prediction==1 else 'Negative',\n",
        "        'probabilities': probabilities,\n",
        "        'model_used': model_name\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "DSoD6Cin8Xw4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_classification():\n",
        "  print(\"\\n\"+\"=\"*60)\n",
        "  print(\"INTERACTIVE GEOTHERMAL POTENTIAL CLASSIFIER\")\n",
        "  print(\"=\"*60)\n",
        "  print(\"Enter feature values to classify geothermal potential\")\n",
        "  print(\"Press Ctrl+C to exit\")\n",
        "\n",
        "  while True:\n",
        "    try:\n",
        "      features={}\n",
        "      print(f\"\\nEnter values for {len(feature_names)} features:\")\n",
        "\n",
        "      for feature in feature_names:\n",
        "        while True:\n",
        "          try:\n",
        "            value=input(f\"{feature}:\")\n",
        "            features[feature]=float(value)\n",
        "            break\n",
        "          except ValueError:\n",
        "            print(\"Please enter a valid number\")\n",
        "\n",
        "      #Get predictions from all models\n",
        "      print(\"\\nPrediction Results:\")\n",
        "      print(\"-\"*40)\n",
        "\n",
        "      for model_name in models.keys():\n",
        "        result=predict_single_sample(features, model_name)\n",
        "        print(f\"{model_name.replace('_', ' ').title()}: {result['prediction_label']}\")\n",
        "\n",
        "        if result['probabilities'] is not None:\n",
        "          prob_negative=result['probabilities'][0]\n",
        "          prob_positive=result['probabilities'][1]\n",
        "          print(f\"Confidence: {max(prob_negative, prob_positive):.3f}\")\n",
        "\n",
        "      print(\"\\nWould you like to classify another sample? (y/n): \", end=\"\")\n",
        "      if input().lower() !='y':\n",
        "        break\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "      print(\"\\nExiting interactive mode.\")\n",
        "      break\n",
        "    except Exception as e:\n",
        "      print(f\"Error: {str(e)}\")"
      ],
      "metadata": {
        "id": "IyEohaAM_vJG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_full_pipeline(filepath=)"
      ],
      "metadata": {
        "id": "xaGg945WDl8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}